{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import re\n",
    "from collections import Counter\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import add\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D,Convolution1D\n",
    "from keras.layers import MaxPool1D,MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.objectives import binary_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "word_counter = Counter()\n",
    "model_path = \"../models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \", string)\n",
    "    string = re.sub(r\"\\)\", \" \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"'\", \" \", string)\n",
    "    string = re.sub(r\"\\[\", \" \", string)\n",
    "    string = re.sub(r\"\\]\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_count(dict_file):\n",
    "    print (type(dict_file))\n",
    "    count = 0\n",
    "    for line in dict_file:\n",
    "        for word in (clean_str(line)).split():\n",
    "            if word not in word_dict.keys():\n",
    "                word_counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n",
      "<class '_io.TextIOWrapper'>\n"
     ]
    }
   ],
   "source": [
    "with open(\"./../fishing_data/pos.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "    create_count(pos_file)\n",
    "with open(\"./../fishing_data/neg.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "    create_count(pos_file)\n",
    "with open(\"./../fishing_data/test/pos.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "    create_count(pos_file)\n",
    "with open(\"./../fishing_data/test/neg.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "     create_count(pos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for word in word_counter.most_common():\n",
    "    count += 1\n",
    "    word_dict[word[0]] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_mapper(input_file):\n",
    "    output_string = []\n",
    "    for line in input_file:\n",
    "        output_line = []\n",
    "        for word in (clean_str(line)).split():\n",
    "            output_line.append(word_dict[word])\n",
    "        output_string.append(output_line)\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pos = [];\n",
    "train_neg = [];\n",
    "test_pos = [];\n",
    "test_neg = [];\n",
    "\n",
    "labels_test = [];\n",
    "with open(\"./../fishing_data/pos.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "    train_pos = line_mapper(pos_file)\n",
    "with open(\"./../fishing_data/neg.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "    train_neg = line_mapper(pos_file)\n",
    "with open(\"./../fishing_data/test/pos.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "    test_pos = line_mapper(pos_file)\n",
    "with open(\"./../fishing_data/test/neg.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "    test_neg = line_mapper(pos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_train_pos = [1] * len(train_pos)\n",
    "labels_train_neg = [0] * len(train_neg)\n",
    "labels_test_pos = [1] * len(test_pos)\n",
    "labels_test_neg = [0] * len(test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of train positive 4338\n",
      "the number of train negative 4392\n",
      "the number of test positive 722\n",
      "the number of test negative 732\n"
     ]
    }
   ],
   "source": [
    "print(\"the number of train positive\",len(train_pos))\n",
    "print(\"the number of train negative\",len(train_neg))\n",
    "print(\"the number of test positive\",len(test_pos))\n",
    "print(\"the number of test negative\",len(test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of training examples 8730\n",
      "the total number of trainng labels 8730\n",
      "the total number of testing examples 1454\n",
      "the total number of testing labels 1454\n"
     ]
    }
   ],
   "source": [
    "train_set = train_pos + train_neg\n",
    "test_set = test_pos + test_neg\n",
    "train_labels =  labels_train_pos + labels_train_neg\n",
    "test_labels = labels_test_pos + labels_test_neg\n",
    "print(\"the total number of training examples\",len(train_set))\n",
    "print(\"the total number of trainng labels\",len(train_labels))\n",
    "print(\"the total number of testing examples\",len(test_set))\n",
    "print(\"the total number of testing labels\",len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "total_data = train_set + test_set;\n",
    "total_labels = train_labels + test_labels;\n",
    "\n",
    "#trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in train_set]\n",
    "#test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in test_set]\n",
    "t_data = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in total_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10184\n"
     ]
    }
   ],
   "source": [
    "print(len(t_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(t_data, total_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimum length is: 2\n",
      "the maximum length is: 1012\n",
      "the average length is: 96.83356245090337\n"
     ]
    }
   ],
   "source": [
    "min_len = min(map(len, t_data))\n",
    "max_len = max(map(len,t_data))\n",
    "avg_len = sum(map(len,t_data)) / len(t_data)\n",
    "print(\"the minimum length is:\",min_len)\n",
    "print(\"the maximum length is:\",max_len)\n",
    "print(\"the average length is:\",avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "\n",
    "trn = sequence.pad_sequences(X_train, maxlen=seq_len, value=0)\n",
    "val = sequence.pad_sequences(X_val, maxlen=seq_len, value=0)\n",
    "test = sequence.pad_sequences(X_test, maxlen=seq_len, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4989, 100)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 200, input_length=seq_len),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 200)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               2000100   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,000,201.0\n",
      "Trainable params: 3,000,201.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4989 samples, validate on 2139 samples\n",
      "Epoch 1/5\n",
      "4989/4989 [==============================] - 1s - loss: 0.4081 - acc: 0.8104 - val_loss: 0.2418 - val_acc: 0.9126\n",
      "Epoch 2/5\n",
      "4989/4989 [==============================] - 1s - loss: 0.1193 - acc: 0.9585 - val_loss: 0.2494 - val_acc: 0.9098\n",
      "Epoch 3/5\n",
      "4989/4989 [==============================] - 1s - loss: 0.0318 - acc: 0.9928 - val_loss: 0.2768 - val_acc: 0.9056\n",
      "Epoch 4/5\n",
      "4989/4989 [==============================] - 1s - loss: 0.0093 - acc: 0.9982 - val_loss: 0.3096 - val_acc: 0.9121\n",
      "Epoch 5/5\n",
      "4989/4989 [==============================] - 1s - loss: 0.0033 - acc: 0.9994 - val_loss: 0.3499 - val_acc: 0.9051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x282a30d37b8>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, y_train, validation_data=(val, y_val), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/3056 [========================>.....] - ETA: 0sthe accuracy on test set is: 0.889397906071\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(test, y_test, batch_size=64);\n",
    "print(\"the accuracy on test set is:\",loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path + 'dense.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv1 = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Convolution1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4989 samples, validate on 3056 samples\n",
      "Epoch 1/6\n",
      "4989/4989 [==============================] - 1s - loss: 0.5196 - acc: 0.7236 - val_loss: 0.2775 - val_acc: 0.8973\n",
      "Epoch 2/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.2226 - acc: 0.9204 - val_loss: 0.2566 - val_acc: 0.9025\n",
      "Epoch 3/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.1614 - acc: 0.9465 - val_loss: 0.2585 - val_acc: 0.9051\n",
      "Epoch 4/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.1070 - acc: 0.9645 - val_loss: 0.3092 - val_acc: 0.8986\n",
      "Epoch 5/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.0862 - acc: 0.9743 - val_loss: 0.3252 - val_acc: 0.9022\n",
      "Epoch 6/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.0555 - acc: 0.9856 - val_loss: 0.3790 - val_acc: 0.8969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x282a4e11be0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, y_train, validation_data=(test, y_test), epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368/3056 [======================>.......] - ETA: 0sthe accuracy on test set is: 0.89692408377\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = conv1.evaluate(test, y_test, batch_size=64);\n",
    "print(\"the accuracy on test set is:\",loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path + 'simple_conv.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained weights with glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "seq_len = 100\n",
    "vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_glove_index(loc):\n",
    "    f = open(loc,encoding=\"utf8\")\n",
    "    embeddings_index = {}\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings_index = load_glove_index(\"../glove/\" + 'glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.18000013e-01,   2.49679998e-01,  -4.12420005e-01,\n",
       "         1.21699996e-01,   3.45270008e-01,  -4.44569997e-02,\n",
       "        -4.96879995e-01,  -1.78619996e-01,  -6.60229998e-04,\n",
       "        -6.56599998e-01,   2.78430015e-01,  -1.47670001e-01,\n",
       "        -5.56770027e-01,   1.46579996e-01,  -9.50950012e-03,\n",
       "         1.16579998e-02,   1.02040000e-01,  -1.27920002e-01,\n",
       "        -8.44299972e-01,  -1.21809997e-01,  -1.68009996e-02,\n",
       "        -3.32789987e-01,  -1.55200005e-01,  -2.31309995e-01,\n",
       "        -1.91809997e-01,  -1.88230002e+00,  -7.67459989e-01,\n",
       "         9.90509987e-02,  -4.21249986e-01,  -1.95260003e-01,\n",
       "         4.00710011e+00,  -1.85939997e-01,  -5.22870004e-01,\n",
       "        -3.16810012e-01,   5.92130003e-04,   7.44489999e-03,\n",
       "         1.77780002e-01,  -1.58969998e-01,   1.20409997e-02,\n",
       "        -5.42230010e-02,  -2.98709989e-01,  -1.57490000e-01,\n",
       "        -3.47579986e-01,  -4.56370004e-02,  -4.42510009e-01,\n",
       "         1.87849998e-01,   2.78489990e-03,  -1.84110001e-01,\n",
       "        -1.15139998e-01,  -7.85809994e-01], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_dict.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=seq_len,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(seq_len,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Convolution1D(64, 5, activation='relu',padding='same')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 100, 50)           250000    \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 100, 64)           16064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 20, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 100)               128100    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 394,265.0\n",
      "Trainable params: 144,265.0\n",
      "Non-trainable params: 250,000.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv2 = Model(sequence_input, preds)\n",
    "conv2.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "conv2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4989 samples, validate on 3056 samples\n",
      "Epoch 1/6\n",
      "4989/4989 [==============================] - 2s - loss: 0.3837 - acc: 0.8286 - val_loss: 0.3116 - val_acc: 0.8940\n",
      "Epoch 2/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.2505 - acc: 0.9030 - val_loss: 0.2769 - val_acc: 0.8884\n",
      "Epoch 3/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.1950 - acc: 0.9246 - val_loss: 0.2666 - val_acc: 0.8917\n",
      "Epoch 4/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.1479 - acc: 0.9445 - val_loss: 0.3024 - val_acc: 0.8969\n",
      "Epoch 5/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.1000 - acc: 0.9651 - val_loss: 0.3449 - val_acc: 0.8819\n",
      "Epoch 6/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.0520 - acc: 0.9848 - val_loss: 0.3831 - val_acc: 0.8878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28299f9d748>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.fit(trn, y_train, validation_data=(test, y_test), epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[1].trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4989 samples, validate on 3056 samples\n",
      "Epoch 1/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.0310 - acc: 0.9942 - val_loss: 0.3885 - val_acc: 0.8946\n",
      "Epoch 2/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.0141 - acc: 0.9974 - val_loss: 0.4452 - val_acc: 0.8923\n",
      "Epoch 3/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.0078 - acc: 0.9988 - val_loss: 0.4873 - val_acc: 0.8973\n",
      "Epoch 4/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.0045 - acc: 0.9994 - val_loss: 0.5410 - val_acc: 0.8933\n",
      "Epoch 5/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.0026 - acc: 0.9998 - val_loss: 0.5553 - val_acc: 0.8930\n",
      "Epoch 6/6\n",
      "4989/4989 [==============================] - 0s - loss: 0.0018 - acc: 0.9998 - val_loss: 0.5801 - val_acc: 0.8953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28299fa6e10>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.fit(trn, y_train, validation_data=(test, y_test), epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-size CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_in = Input ((vocab_size, 50))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Convolution1D(64, fsz, padding='same', activation=\"relu\")(graph_in)\n",
    "    x = MaxPooling1D()(x) \n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = concatenate(convs) \n",
    "graph = Model(graph_in, out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv3 = Sequential ([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, weights=[embedding_matrix]),\n",
    "    SpatialDropout1D(0.2),\n",
    "    graph,\n",
    "    Dropout (0.5),\n",
    "    Dense (100, activation=\"relu\"),\n",
    "    Dropout (0.7),\n",
    "    Dense (1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv3.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4989 samples, validate on 3056 samples\n",
      "Epoch 1/10\n",
      "4989/4989 [==============================] - 1s - loss: 0.1503 - acc: 0.9435 - val_loss: 0.2561 - val_acc: 0.9103\n",
      "Epoch 2/10\n",
      "4989/4989 [==============================] - 1s - loss: 0.1248 - acc: 0.9517 - val_loss: 0.2894 - val_acc: 0.9139\n",
      "Epoch 3/10\n",
      "4989/4989 [==============================] - 1s - loss: 0.1227 - acc: 0.9513 - val_loss: 0.2699 - val_acc: 0.9149\n",
      "Epoch 4/10\n",
      "4989/4989 [==============================] - 1s - loss: 0.1005 - acc: 0.9617 - val_loss: 0.2769 - val_acc: 0.9100\n",
      "Epoch 5/10\n",
      "4989/4989 [==============================] - 1s - loss: 0.0897 - acc: 0.9701 - val_loss: 0.2922 - val_acc: 0.9175\n",
      "Epoch 6/10\n",
      "4989/4989 [==============================] - 1s - loss: 0.0888 - acc: 0.9667 - val_loss: 0.3143 - val_acc: 0.9195\n",
      "Epoch 7/10\n",
      "4989/4989 [==============================] - 1s - loss: 0.0702 - acc: 0.9757 - val_loss: 0.3491 - val_acc: 0.9192\n",
      "Epoch 8/10\n",
      "4989/4989 [==============================] - 1s - loss: 0.0673 - acc: 0.9737 - val_loss: 0.3413 - val_acc: 0.9179\n",
      "Epoch 9/10\n",
      "4989/4989 [==============================] - 1s - loss: 0.0629 - acc: 0.9755 - val_loss: 0.3422 - val_acc: 0.9152\n",
      "Epoch 10/10\n",
      "4989/4989 [==============================] - 1s - loss: 0.0562 - acc: 0.9800 - val_loss: 0.3772 - val_acc: 0.9149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2829d5cae48>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3.fit(trn, y_train, validation_data=(test, y_test), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816/3056 [==========================>...] - ETA: 0sthe accuracy on test set is: 0.914921466593\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = conv3.evaluate(test, y_test, batch_size=64);\n",
    "print(\"the accuracy on test set is:\",loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better ways to preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "EMBEDDING_DIM = 50\n",
    "seq_len = 100\n",
    "vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_lines(file):\n",
    "    count = 0\n",
    "    lines = [];\n",
    "    for line in file:\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "pos_tr = []\n",
    "neg_tr = []\n",
    "\n",
    "pos_te = []\n",
    "neg_te = []\n",
    "\n",
    "\n",
    "with open(\"./../fishing_data/pos.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "    pos_tr = extract_lines(pos_file)\n",
    "with open(\"./../fishing_data/neg.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "    neg_tr = extract_lines(pos_file)\n",
    "with open(\"./../fishing_data/test/pos.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "    pos_te = extract_lines(pos_file)\n",
    "with open(\"./../fishing_data/test/neg.txt\", \"r\", encoding=\"utf8\") as pos_file:\n",
    "     neg_te = extract_lines(pos_file)\n",
    "\n",
    "all_text = pos_tr + neg_tr + pos_te + neg_te\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "sequences = tokenizer.texts_to_sequences(all_text)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 294,\n",
       " 82,\n",
       " 47,\n",
       " 11,\n",
       " 40,\n",
       " 5,\n",
       " 100,\n",
       " 151,\n",
       " 109,\n",
       " 13,\n",
       " 14,\n",
       " 2255,\n",
       " 8,\n",
       " 7,\n",
       " 19,\n",
       " 27,\n",
       " 197,\n",
       " 23,\n",
       " 109,\n",
       " 12,\n",
       " 17,\n",
       " 2364,\n",
       " 113,\n",
       " 81,\n",
       " 3,\n",
       " 117,\n",
       " 101,\n",
       " 12,\n",
       " 671,\n",
       " 131,\n",
       " 61,\n",
       " 5,\n",
       " 11,\n",
       " 327,\n",
       " 47,\n",
       " 119,\n",
       " 6,\n",
       " 146,\n",
       " 5,\n",
       " 15,\n",
       " 294,\n",
       " 48,\n",
       " 4,\n",
       " 5,\n",
       " 15,\n",
       " 294,\n",
       " 3382,\n",
       " 9,\n",
       " 59,\n",
       " 13,\n",
       " 14,\n",
       " 59,\n",
       " 8,\n",
       " 2255,\n",
       " 252,\n",
       " 1998,\n",
       " 2769,\n",
       " 3245,\n",
       " 104,\n",
       " 2710,\n",
       " 2495,\n",
       " 1376,\n",
       " 5,\n",
       " 15,\n",
       " 294,\n",
       " 701,\n",
       " 545,\n",
       " 47,\n",
       " 20,\n",
       " 575,\n",
       " 47,\n",
       " 20,\n",
       " 594,\n",
       " 61,\n",
       " 20,\n",
       " 594,\n",
       " 4250,\n",
       " 575,\n",
       " 47,\n",
       " 4250,\n",
       " 545,\n",
       " 20,\n",
       " 20,\n",
       " 4250,\n",
       " 545,\n",
       " 12,\n",
       " 20,\n",
       " 47,\n",
       " 20,\n",
       " 1596,\n",
       " 327,\n",
       " 141,\n",
       " 35,\n",
       " 15,\n",
       " 923,\n",
       " 569]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ufeffFishing with Rod: BC Stillwater Trout For more fishing videos, please visit: http://www.fishingwithrod.com    In this video blog, we visit Onion Lake at Ruddocks Ranch just north of Lytton, British Columbia. Onion Lake offers good stillwater fly fishing for rainbow trout.    Subscribe to go Fishing with Rod!    Be a Fishing with Rod fan on Facebook:  http://www.facebook.com/fishingwithrod    Camera: Nina Manique  Edit: Rodney Hsu  Music: Getty Images Inc.  Copyright: Fishing with Rod Production ['BC trout fishing', 'rainbow trout fishing', 'trout fly fishing', 'trout flyfishing', 'rainbow trout flyfishing', 'BC stillwater fishing', 'chironomid fishing', 'damsel flyfishing', 'BC lake fishing', 'stillwater trout fishing', 'trophy rainbow trout', 'Fishing with Rod', 'fishingwithrod.com']\\n\""
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fishing\n",
      "with\n",
      "rod\n",
      "bc\n",
      "stillwater\n",
      "trout\n",
      "for\n",
      "more\n",
      "fishing\n",
      "videos\n",
      "please\n",
      "visit\n",
      "http\n",
      "www\n",
      "fishingwithrod\n",
      "com\n",
      "in\n",
      "this\n",
      "video\n",
      "blog\n",
      "we\n",
      "visit\n",
      "onion\n",
      "lake\n",
      "at\n",
      "ruddocks\n",
      "ranch\n",
      "just\n",
      "north\n",
      "of\n",
      "lytton\n",
      "british\n",
      "columbia\n",
      "onion\n",
      "lake\n",
      "offers\n",
      "good\n",
      "stillwater\n",
      "fly\n",
      "fishing\n",
      "for\n",
      "rainbow\n",
      "trout\n",
      "subscribe\n",
      "to\n",
      "go\n",
      "fishing\n",
      "with\n",
      "rod\n",
      "!\n",
      "be\n",
      "a\n",
      "fishing\n",
      "with\n",
      "rod\n",
      "fan\n",
      "on\n",
      "facebook\n",
      "http\n",
      "www\n",
      "facebook\n",
      "com\n",
      "fishingwithrod\n",
      "camera\n",
      "nina\n",
      "manique\n",
      "edit\n",
      "rodney\n",
      "hsu\n",
      "music\n",
      "getty\n",
      "images\n",
      "inc\n",
      "copyright\n",
      "fishing\n",
      "with\n",
      "rod\n",
      "production\n",
      "bc\n",
      "trout\n",
      "fishing\n",
      "rainbow\n",
      "trout\n",
      "fishing\n",
      "trout\n",
      "fly\n",
      "fishing\n",
      "trout\n",
      "flyfishing\n",
      "rainbow\n",
      "trout\n",
      "flyfishing\n",
      "bc\n",
      "stillwater\n",
      "fishing\n",
      "chironomid\n",
      "fishing\n",
      "damsel\n",
      "flyfishing\n",
      "bc\n",
      "lake\n",
      "fishing\n",
      "stillwater\n",
      "trout\n",
      "fishing\n",
      "trophy\n",
      "rainbow\n",
      "trout\n",
      "fishing\n",
      "with\n",
      "rod\n",
      "fishingwithrod\n",
      "com\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ML]",
   "language": "python",
   "name": "Python [ML]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
